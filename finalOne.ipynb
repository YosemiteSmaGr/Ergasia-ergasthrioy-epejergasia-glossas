{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c3e3aa",
   "metadata": {},
   "source": [
    "## ***ΕΠΕΞΕΡΓΑΣΙΑ ΦΥΣΙΚΗΣ ΓΛΩΣΣΑΣ ΚΑΙ ΣΗΜΑΣΙΟΛΟΓΙΚΟΣ ΙΣΤΟΣ***\n",
    "ΕΡΓΑΣIΑ ΕΞΑΜΝΟΥ 2024-2025 <br>\n",
    "***Κωνσταντίνος Ιωάννης Σιατερλής 18390192***\n",
    "\n",
    "Το πρόβλημα είναι να μπορέσει μια μηχανή να αναγνώριση αν ένα κείμενο είναι spam η όχι \n",
    "\n",
    "Αρχικά με την βοήθεια της βιβλιοθήκης numpy διαβάζω τα δεδομένα με καταλαλητά κωδικοποίηση. Η numpy διαβάζει όλα τα αλφαριθμητικά σαν ένα κομματικά με την χρήση του quotechar στο numpy.loadtxt, επίσης αρχικοποιώ μερικές μεταβλητές για ευκολία.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5937ad8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ham'\n",
      "  'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
      " ['ham' 'Ok lar... Joking wif u oni...']\n",
      " ['spam'\n",
      "  \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n",
      " ['ham' 'U dun say so early hor... U c already then say...']\n",
      " ['ham' \"Nah I don't think he goes to usf, he lives around here though\"]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DATA = np.loadtxt(\"./mail_data.csv\", delimiter=\",\", dtype=str, usecols=(0,1), encoding ='utf-8', quotechar='\"', skiprows=1)\n",
    "#np.random.shuffle(DATA)\n",
    "\n",
    "SPAMMAP = {'ham': 0, 'spam': 1}\n",
    "STOPWORDS = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "TRAINSIZE = 0.2 \n",
    "\n",
    "print(DATA[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc70e9b",
   "metadata": {},
   "source": [
    "# **Preperation**\n",
    "\n",
    "Για να μπορέσω αναγνωρίσω αν ειναι spam η όχι  πρώτα πρέπει να επεξεργαστώ της προτάσεις. Η συντήρηση preparation έχει δημιουργηθεί για αυτόν τον λόγο δηλαδή :\n",
    "\n",
    "* Κανονικοποίηση Κειμένου\n",
    "* Διαίρεση σε Λέξεις\n",
    "* Αφαίρεση Λέξεων\n",
    "* Ανάλυση Μορφολογίας\n",
    "* Μέρος του Λόγου\n",
    "\n",
    "Επιστρέφει ενα bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a318424-528e-4909-bcb4-fe60ac4f17d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'go_VB': True,\n",
       "  'jurong_JJ': True,\n",
       "  'point_NN': True,\n",
       "  'crazi_NN': True,\n",
       "  'avail_NN': True,\n",
       "  'bugi_NN': True,\n",
       "  'n_RB': True,\n",
       "  'great_JJ': True,\n",
       "  'world_NN': True,\n",
       "  'la_NN': True,\n",
       "  'e_VBP': True,\n",
       "  'buffet_JJ': True,\n",
       "  'cine_NN': True,\n",
       "  'got_VBD': True,\n",
       "  'amor_JJ': True,\n",
       "  'wat_NN': True},\n",
       " 'ham')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preparation(str_, stopWords):\n",
    "    finalList =[]\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "    str_.lower()\n",
    "    str_ = str_.replace(\"-\", \" \")\n",
    "\n",
    "    finalList = nltk.tokenize.word_tokenize(str_)\n",
    "    finalList = [\"\".join( ch for ch in w if ch.isalnum() ) for w in finalList if w not in stopWords]\n",
    "    finalList = [ps.stem(w) for w in finalList ]\n",
    "    finalList = [w for w in finalList if w != \"\"]\n",
    "\n",
    "    tags = nltk.pos_tag(finalList)\n",
    "    finalFeatures = {}\n",
    "\n",
    "    # Bag the words\n",
    "    for word, tag in tags:\n",
    "        finalFeatures[f'{word}_{tag}'] = True\n",
    "\n",
    "    return finalFeatures\n",
    "\n",
    "finalDataSet = []\n",
    "\n",
    "for i in DATA:\n",
    "    finalDataSet.append((preparation(i[1], STOPWORDS), i[0]))\n",
    "\n",
    "finalDataSet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d38c8-b7b6-4707-988e-fc0d732cf3cb",
   "metadata": {},
   "source": [
    "Για την εκπαίδευση του αλγόριθμου πρέπει να χωρίσω \n",
    "τα δεδομένα σε εκπαίδευσης και δοκιμής.\n",
    "\n",
    "Με το len περνώ το 20% του του τέλους και το 80% της αρχής"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c5bef7-9841-4839-8d62-f4aaca78d1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data : 1114\n",
      "Length of test data : 4458 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSizeSentence = max(len(x[0]) for x in finalDataSet)\n",
    "sizePerSentence = [len(x[0]) for x in finalDataSet]\n",
    "avgSizeSentence = int(sum(sizePerSentence) / len(sizePerSentence))\n",
    "\n",
    "# train test split, 80 - 20\n",
    "trainData = finalDataSet[:(int(len(finalDataSet) * TRAINSIZE))]\n",
    "testData = finalDataSet[(int(len(finalDataSet) * TRAINSIZE)):]\n",
    "\n",
    "print(\"Length of training data :\", len(trainData))\n",
    "print(\"Length of test data :\", len(testData), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b9d36",
   "metadata": {},
   "source": [
    "# **Naive Bayes**\n",
    "Ο Naive Bayes είναι ένας πιθανοκρατικός αλγόριθμος μηχανικής μάθησης βασισμένος στο Θεώρημα του Bayes ο οποίος κάνει μια υπόθεση ότι όλα τα χαρακτηριστικά είναι ανεξάρτητα μεταξύ τους δεδομένης της κατηγορίας.\n",
    "\n",
    "Αλγόριθμος naive bayes υπάρχει έτυμος στην βιβλιοθήκη NLTK οπου παίρνει σαν είσοδο ένα πινάκα από bag of words.\n",
    "Υστερα προβλέπει τα δεδομένα εκπαίδευσης αν είναι spam η όχι και τα βάζει μέσα σε ένα πίνακα.\n",
    "\n",
    "Με την classification_report (sklearn.metrics) βγάζει μια αναφορά ακρίβειας (F1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14849e11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.83      0.91      3879\n",
      "        spam       0.47      0.97      0.63       579\n",
      "\n",
      "    accuracy                           0.85      4458\n",
      "   macro avg       0.73      0.90      0.77      4458\n",
      "weighted avg       0.93      0.85      0.87      4458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testNativeBytes = nltk.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "yPred = [testNativeBytes.classify(x[0]) for x in testData]\n",
    "yTest = [x[1] for x in testData]\n",
    "\n",
    "print(classification_report(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19196338-b6af-40c7-bf5e-50006cfd0983",
   "metadata": {},
   "source": [
    "Ο αλγόριθμος Naive Bayes είναι ιδιαίτερα αποδοτικός στην κατηγοριοποίηση κειμένου. Λειτουργεί καλά με πολλά δεδομένα είναι γρήγορος στην εκπαίδευση και πρόβλεψη. Παρά την απλότητά του έχει αρκετούς περιορισμούς υποθέτει την ανεξαρτησίας μεταξύ των χαρακτηριστικών η οποία σπάνια ισχύει."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7a5b1",
   "metadata": {},
   "source": [
    "# **Word To Vector και Logistic Regression**\n",
    "\n",
    "Ο Word2Vec είναι ένα αλγόριθμο νευρωνικού δίκτυου που εκπαιδεύεται έτσι ώστε οι λέξεις που εμφανίζονται με παρόμοια συμφραζόμενα να αποκτούν παρόμοια διανύσματα. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4db3f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go_VB',\n",
       "  'jurong_JJ',\n",
       "  'point_NN',\n",
       "  'crazi_NN',\n",
       "  'avail_NN',\n",
       "  'bugi_NN',\n",
       "  'n_RB',\n",
       "  'great_JJ',\n",
       "  'world_NN',\n",
       "  'la_NN',\n",
       "  'e_VBP',\n",
       "  'buffet_JJ',\n",
       "  'cine_NN',\n",
       "  'got_VBD',\n",
       "  'amor_JJ',\n",
       "  'wat_NN'],\n",
       " ['ok_JJ', 'lar_JJ', 'joke_NN', 'wif_NN', 'u_JJ', 'oni_NN']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "\n",
    "scamOrNot = np.array([mail[1] for mail in finalDataSet])\n",
    "\n",
    "tmpSentences = [list(mail[0].keys()) for mail in finalDataSet]\n",
    "tmpSentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac8fde",
   "metadata": {},
   "source": [
    "Αλγόριθμος word2vec υπάρχει έτυμος στην βιβλιοθήκη gensim, περνει σαν εισοδο τις προτασεις και τα μεγεθει του vector που το καθε ενα απο αυτα θα χαρακτηρίζει μια λέξει.\n",
    "\n",
    "Η συνάρτηση WordsToVec μετατρέπει μια πρόταση σε ένα σύνολα από πίνακες με τα αντίστοιχα vectors των λέξεων αυτής της πρότασης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a7c8ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def WordsToVec(mail):\n",
    "    tmpVec = [model.wv[word] for word in mail if word in model.wv]\n",
    "    if len(tmpVec) != 0:\n",
    "        return tmpVec\n",
    "    return []\n",
    "\n",
    "# Montelo Word2vercor\n",
    "model = gensim.models.Word2Vec(sentences=tmpSentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Lejeis se areiumoys\n",
    "wordVecNumbers = [WordsToVec(mail) for mail in tmpSentences]\n",
    "mymax = max(len(x) for x in wordVecNumbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436ebfa",
   "metadata": {},
   "source": [
    "Για να αναγνωριστούν η πρότασης που είναι spam χρησιμοποιείται η Logistic Regression, αλλά η  Logistic Regression παίρνει σαν είσοδο ένα πινάκα δυο άξονες, αν είναι spam η όχι και το κείμενο, για αυτό τον λόγο ενώνω όλα τα vector των λέξεων σε ένα. Τελος βγάζει μια αναφορά ακρίβειας (F1-score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22254c6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.90      0.98      0.94       972\n",
      "        spam       0.68      0.29      0.41       143\n",
      "\n",
      "    accuracy                           0.89      1115\n",
      "   macro avg       0.79      0.64      0.68      1115\n",
      "weighted avg       0.88      0.89      0.87      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# προσθηκη κενών για ομογένεια\n",
    "for x in wordVecNumbers:\n",
    "    x += [[0] * 100 for _ in range(mymax - len(x))]\n",
    "\n",
    "# Μεταφραση του πινακα σε numpy\n",
    "wordVecNumbers = np.array(wordVecNumbers)\n",
    "\n",
    "# Μετατροπη του πινακα σε 2 άξονες\n",
    "wordVecNumbers = wordVecNumbers.reshape(wordVecNumbers.shape[0], -1)\n",
    "\n",
    "# 80 - 20\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(wordVecNumbers, scamOrNot, test_size=TRAINSIZE)\n",
    "\n",
    "# LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(XTrain, yTrain)\n",
    "\n",
    "yPred = clf.predict(XTest)\n",
    "print(classification_report(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad0540-8fba-43b6-8ebc-cebfbb1455a7",
   "metadata": {},
   "source": [
    "Ο συνδυασμός Word2Vec + Classifier είναι μια πολύ ισχυρή προσέγγιση για κατανόηση και κατηγοριοποίηση κειμένου. Το Word2Vec μετατρέπει τις λέξεις σε σημασιολογικά πλούσια διανύσματα που συλλαμβάνουν σχέσεις μεταξύ λέξεων ενώ ένας Classifier κατηγοριοποιεί τα δεδομένα με βάση αυτά τα διανύσματα. Ωστόσο ο Word2Vec παράγει ένα σταθερό διάνυσμα ανά λέξη ανεξαρτήτως του πλαισίου πχ \"τράπεζα αίματος\" είτε για \"τράπεζα χρήματος\" και δεν λαμβάνει υπόψη τη συντακτική σειρά ή τη γραμματική της πρότασης "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00127c77",
   "metadata": {},
   "source": [
    "# **Recurrent Neural Networks - RNNs**\n",
    "Ένα επαναλαμβανόμενο νευρωνικό δίκτυο είναι ένα μοντέλο μάθησης που έχει εκπαιδευτεί να επεξεργάζεται και να μετατρέπει μια διαδοχική είσοδο δεδομένων σε μια συγκεκριμένη διαδοχική έξοδο δεδομένων.\n",
    "\n",
    "Η pyporch έχει όλα τα απαραίτητα εργαλεία για να ένα RNN, πρώτα ώμος χρειάζεται ένα λεξικό και επειδή δεν μπορεί να καταλάβει λέξη τις χαρακτηρίζονται απο ένα άκαιρος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac2323c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#maxSizeSentence avgSizeSentence\n",
    "SIZEOFRNNS = avgSizeSentence\n",
    "\n",
    "# vocabulary\n",
    "allFeatures = set()\n",
    "for features, _ in trainData:\n",
    "    allFeatures.update(features.keys())\n",
    "\n",
    "# +1 to reserve 0 for padding\n",
    "featureAndIdx = {word: i + 1 for i, word in enumerate(sorted(allFeatures))}\n",
    "vocSize = len(featureAndIdx) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2b8c4",
   "metadata": {},
   "source": [
    "Για να μπόρεση να αναγνώριση τα δεδομένα ένας αλγόριθμος RNN πρέπει να φταίχτη ένας συγκεκριμένος τύπος δεδομένον, που είναι παιδί του Dataset αντικείμενου από την pytorch.\n",
    "\n",
    "Η encFeatures αλλάζει τις λέξις με τα αντίστοιχα αναγνωριστικά τους και προσθετή κενά για ομογένεια."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135ba728",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def encFeatures(featureDict, maxLen = SIZEOFRNNS):\n",
    "    indices = [featureAndIdx[k] for k in featureDict if k in featureAndIdx]\n",
    "    indices = indices[:maxLen]\n",
    "    indices += [0] * (maxLen - len(indices))\n",
    "    return indices\n",
    "\n",
    "# DataSet RNNS\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, data, maxLen = SIZEOFRNNS):\n",
    "        self.X = [encFeatures(x, maxLen) for x, _ in data]\n",
    "        self.y = [SPAMMAP[y] for _, y in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f7ad7",
   "metadata": {},
   "source": [
    "Το Νευρωνικό δίκτυο είναι ένα βασικό δίκτυο RNNs με νευρώνες εισόδου όσο το μέσο όρο των λέξεων στων προτάσεων, κρυφά επίπεδα 16 και έξοδο μια γραμμικού τύπο συνάρτηση με μέγιστο το 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1891c3f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, h = self.rnn(x)\n",
    "        out = self.fc(h.squeeze(0))\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7151d6",
   "metadata": {},
   "source": [
    "Πέρασμα των δεδομένον εκπαίδευσης και αρχικοποιήσει μεταβλητών για διόρθωση σφάλματος οπός το κριτήριο σφάλματος (criterion) και ο ρυθμός μάθησης του αλγορίθμου (optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96443950",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Epoch 1, Loss: 0.1298\n",
      "Epoch 2, Loss: 2.3247\n",
      "Epoch 3, Loss: 0.0859\n",
      "Epoch 4, Loss: 0.0052\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(SIZEOFRNNS)\n",
    "dataset = myDataset(trainData)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model = RNNClassifier(vocab_size=vocSize, embed_size=SIZEOFRNNS, hidden_size=16)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(4):\n",
    "    for x, y in loader:\n",
    "        yPred = model(x).squeeze().unsqueeze(0)\n",
    "        loss = criterion(yPred, y)\n",
    "\n",
    "        # Clean\n",
    "        optimizer.zero_grad()\n",
    "        # Computes the loss\n",
    "        loss.backward()\n",
    "        # Updates the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f12444",
   "metadata": {},
   "source": [
    "Πρόβλεψη των προτάσεων μεσώ μια συντήρησης και βγάζει μια αναφορά ακρίβειας (F1-score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a330cd2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.93      0.98      0.95      3879\n",
      "        spam       0.78      0.49      0.60       579\n",
      "\n",
      "    accuracy                           0.92      4458\n",
      "   macro avg       0.86      0.73      0.78      4458\n",
      "weighted avg       0.91      0.92      0.91      4458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(featureDict):\n",
    "    x = torch.tensor(encFeatures(featureDict)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    return 'spam' if out.item() > 0.5 else 'ham'\n",
    "\n",
    "yPred = [ predict(x[0]) for x in testData ]\n",
    "yTest = [ str(x[1]) for x in testData ]\n",
    "\n",
    "print(\"\\n\", classification_report(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec4739-7327-44d2-89d9-fb2e04a51b14",
   "metadata": {},
   "source": [
    "Τα RNNs είναι ιδιαίτερα κατάλληλα για σειριακά δεδομένα όπως κείμενο επειδή έχουν “μνήμη” λαμβάνουν υπόψη τους προηγούμενες λέξεις. Αυτό τα καθιστά ισχυρά για εργασίες όπως ανάλυση συναισθήματος και κατηγοριοποίηση προτάσεων, όπου η σειρά των λέξεων έχει σημασία. Ωστόσο, παρουσιάζουν σημαντικά αρνητικά, όπως το πρόβλημα της έκρηξης των βαθμίδων (vanishing/exploding gradients), που τα κάνει δύσκολα στην εκπαίδευση σε μεγάλες ακολουθίες."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
